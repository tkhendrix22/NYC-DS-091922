{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79adf0f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-\\amily:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Regularization\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Nov 2022\n",
    "<p>Phase 3: Topic 24</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfee06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "- **Explain** the bias-variance tradeoff and the correlative notions of underfit and overfit models\n",
    "- Explain the notion of \"validation data\"\n",
    "- Use the algorithm of cross-validation (with `sklearn`)\n",
    "- Explain the concept of regularization\n",
    "- Use Lasso and Ridge regularization in model design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28803a9",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb0c6d",
   "metadata": {},
   "source": [
    "We can break up how the model makes mistakes (the error) by saying there are three parts:\n",
    "\n",
    "- Error inherent in the data (noise): **irreducible error**\n",
    "- Error from not capturing signal (too simple): **bias**\n",
    "- Error from \"modeling noise\", i.e. capturing patterns in the data that don't generalize well (too complex): **variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12730388",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e206e7a",
   "metadata": {},
   "source": [
    "**High-bias** algorithms tend to be less complex, with simple or rigid underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca217438",
   "metadata": {},
   "source": [
    "![](images/noisy-sine-linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3684d",
   "metadata": {},
   "source": [
    "+ They train models that are consistent, but inaccurate on average.\n",
    "+ These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "+ The following sorts of difficulties could lead to high bias:\n",
    "  - We did not include the correct predictors\n",
    "  - We did not take interactions into account\n",
    "  - We missed a non-linear (polynomial) relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd59ee3",
   "metadata": {},
   "source": [
    "      \n",
    "High-bias models are generally **underfit**: The models have not picked up enough of the signal in the data. And so even though they may be consistent, they don't perform particularly well on the initial data, and so they will be consistently inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe451b",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d222f",
   "metadata": {},
   "source": [
    "On the other hand, **high-variance** algorithms tend to be more complex, with flexible underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a416ac",
   "metadata": {},
   "source": [
    "<img src = \"images/noisy-sine-decision-tree.png\"  width = 800/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac917e",
   "metadata": {},
   "source": [
    "+ They train models that are accurate on average, but inconsistent.\n",
    "+ These include non-linear or non-parametric algorithms such as decision trees and nearest-neighbor models.\n",
    "+ The following sorts of difficulties could lead to high variance:\n",
    "  - We included an unreasonably large number of predictors;\n",
    "  - We created new features by squaring and cubing each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af683b07",
   "metadata": {},
   "source": [
    "High variance models are **overfit**: The models have picked up on the noise as well as the signal in the data. And so even though they may perform well on the initial data, they will be inconsistently accurate on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Balancing Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5022e",
   "metadata": {},
   "source": [
    "While we build our models, we have to keep this relationship in mind.  If we build complex models, we risk overfitting our models.  Their predictions will vary greatly when introduced to new data.  If our models are too simple, the predictions as a whole will be inaccurate.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a51d56",
   "metadata": {},
   "source": [
    "![](images/noisy-sine-third-order-polynomial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdba225",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bias: \n",
    "- when model not complex enough\n",
    "- feature space not adequately rich enough to explain target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f24baa",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Variance: \n",
    "\n",
    "- model/weights: large fluctuations about true model given different train sets\n",
    "\n",
    "- High $ \\mathrm{Var}[\\textbf{w}] $ over realization of training sets\n",
    "\n",
    "- High fluctuation in MAE over test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa015c",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The bulls-eye diagrams of fitting model to different training set realizations:\n",
    "<center><img src = \"images/biasvar_bullseye.png\" width = 400/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0893d0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each dot is a model:\n",
    "- Bulls-eye: the *true* model (generating mean of $y$ given $X$ in the population) \n",
    "- Each dot: models trained on different samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4173a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Our goal**: lowering bias and variance in training predictive models\n",
    "\n",
    "but the two often at odds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bb014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multicollinearity\n",
    "Have to grapple with these issues when constructing linear models with multicollinear features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1194921",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We talked about this way back. But how does it increase Var[$\\textbf{w}$]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da55d1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "WHO_data = pd.read_csv(\"data/WHO_life.csv\")\n",
    "X_WHO = WHO_data.drop(columns = [\"Life expectancy \"])\n",
    "y = WHO_data[\"Life expectancy \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dd4f6d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>...</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2015</td>\n",
       "      <td>Developing</td>\n",
       "      <td>263.0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>71.279624</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>19.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.16</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>584.259210</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.479</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2014</td>\n",
       "      <td>Developing</td>\n",
       "      <td>271.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.523582</td>\n",
       "      <td>62.0</td>\n",
       "      <td>492</td>\n",
       "      <td>18.6</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>612.696514</td>\n",
       "      <td>327582.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.476</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>Developing</td>\n",
       "      <td>268.0</td>\n",
       "      <td>66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.219243</td>\n",
       "      <td>64.0</td>\n",
       "      <td>430</td>\n",
       "      <td>18.1</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.13</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>631.744976</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.470</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2012</td>\n",
       "      <td>Developing</td>\n",
       "      <td>272.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>78.184215</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2787</td>\n",
       "      <td>17.6</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>8.52</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>669.959000</td>\n",
       "      <td>3696958.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2011</td>\n",
       "      <td>Developing</td>\n",
       "      <td>275.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.097109</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3013</td>\n",
       "      <td>17.2</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.537231</td>\n",
       "      <td>2978599.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.454</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  Year      Status  Adult Mortality  infant deaths  Alcohol  \\\n",
       "0  Afghanistan  2015  Developing            263.0             62     0.01   \n",
       "1  Afghanistan  2014  Developing            271.0             64     0.01   \n",
       "2  Afghanistan  2013  Developing            268.0             66     0.01   \n",
       "3  Afghanistan  2012  Developing            272.0             69     0.01   \n",
       "4  Afghanistan  2011  Developing            275.0             71     0.01   \n",
       "\n",
       "   percentage expenditure  Hepatitis B  Measles    BMI   ...  Polio  \\\n",
       "0               71.279624         65.0      1154   19.1  ...    6.0   \n",
       "1               73.523582         62.0       492   18.6  ...   58.0   \n",
       "2               73.219243         64.0       430   18.1  ...   62.0   \n",
       "3               78.184215         67.0      2787   17.6  ...   67.0   \n",
       "4                7.097109         68.0      3013   17.2  ...   68.0   \n",
       "\n",
       "   Total expenditure  Diphtheria    HIV/AIDS         GDP  Population  \\\n",
       "0               8.16         65.0        0.1  584.259210  33736494.0   \n",
       "1               8.18         62.0        0.1  612.696514    327582.0   \n",
       "2               8.13         64.0        0.1  631.744976  31731688.0   \n",
       "3               8.52         67.0        0.1  669.959000   3696958.0   \n",
       "4               7.87         68.0        0.1   63.537231   2978599.0   \n",
       "\n",
       "    thinness  1-19 years   thinness 5-9 years  \\\n",
       "0                   17.2                 17.3   \n",
       "1                   17.5                 17.5   \n",
       "2                   17.7                 17.7   \n",
       "3                   17.9                 18.0   \n",
       "4                   18.2                 18.2   \n",
       "\n",
       "   Income composition of resources  Schooling  \n",
       "0                            0.479       10.1  \n",
       "1                            0.476       10.0  \n",
       "2                            0.470        9.9  \n",
       "3                            0.463        9.8  \n",
       "4                            0.454        9.5  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_WHO.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16282345",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many features from WHO dataset:\n",
    "\n",
    "Regressing to find weights life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47166a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country', 'Year', 'Status', 'Adult Mortality', 'infant deaths',\n",
       "       'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles ', ' BMI ',\n",
       "       'under-five deaths ', 'Polio', 'Total expenditure', 'Diphtheria ',\n",
       "       ' HIV/AIDS', 'GDP', 'Population', ' thinness  1-19 years',\n",
       "       ' thinness 5-9 years', 'Income composition of resources', 'Schooling'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_WHO.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4af08a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But let's take a look at a few of these and their correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a98830",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_selector = ['Income composition of resources', 'Schooling','Alcohol', ' thinness  1-19 years']\n",
    "subsetX = X_WHO[col_selector]\n",
    "sns.heatmap(subsetX.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144992f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's focus on Schooling and income composite resources (ICR):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7276cad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ Life= w_1*Alcohol + w_2*Polio + w_3*Schooling + w_4*Measles + w_5*ICR + ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f8e9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Correlation is very high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ba4dd4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Income composition of resources</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schooling</th>\n",
       "      <td>0.800092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Income composition of resources  Schooling\n",
       "Income composition of resources                         1.000000   0.800092\n",
       "Schooling                                               0.800092   1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_selector = ['Income composition of resources', 'Schooling']\n",
    "X_WHO[col_selector].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498e01d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our regression: \n",
    "- Y = life expectancy\n",
    "\n",
    "$$ Y - \\sum_{i \\neq 3,5} w_i x_i = w_3 Schooling + w_5 ICR $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26724a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Schooling and ICR highly related:\n",
    "\n",
    "- Implies that $w_3$ and $w_5$ introduce too much flexibility.\n",
    "- Maybe could fit almost as well with just $w_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89056b45",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $w_3$ and $w_5$ are floppy and can become big in either direction to fit data.\n",
    "- Var[$\\textbf{w}$] from $w_3$ and $w_5$ high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b0e80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Modeling data by linear model w/ multicollinear features:\n",
    "- intoduces high weight variance\n",
    "- unnecessary model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f36b01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These considerations are all nice and theoretical:\n",
    "    \n",
    "- how do we actually assess whether model suffers from bias / variance or both?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09965e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to assess model variance: cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43237dcc",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could get many different training sets:\n",
    "- Train weights $\\textbf{w}$ for each.\n",
    "- Get variance of $\\textbf{w}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb76059",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Semi-equivalently:\n",
    "- Test performance of each model on test set.\n",
    "- Evaluate model performance/variance by looking at average/standard deviation of performance on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d56a10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem: \n",
    "- likely don't have this much data available to make enough independent training sets large enough to for each model to train on effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2b769",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution: Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d5c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So first we created our train / test split: \n",
    "\n",
    "- the **training set** can be used to develop models\n",
    "- can assess variance of a model and average performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cda9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"images/traintestsplit.png\"  width = 800/>\n",
    "<center> Splitting up training set </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b2821",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/crossval.png\"  width = 800/>\n",
    "<center> Splitting up training set </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee35c4d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Split up training set into folds:\n",
    "- Training fold\n",
    "- Validation fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67599a49",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each iteration:\n",
    "    - train a model.\n",
    "    - Test on validation fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a611d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Good for estimating model performance on average\n",
    "- Good for estimating model variance as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a0ea6",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Effectively sampling multiple training sets:\n",
    "- testing each model performance on different **validation set**.\n",
    "- Good for estimating model performance on average\n",
    "- Good for estimating model variance as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02b829",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So in the end:\n",
    "- Performance metrics measured on validation\n",
    "- We get average performance metric across all the models for each cross validation iteration.\n",
    "- Get variance of performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39672d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: **validation set** is part of training set:\n",
    "- Not part of true test/hold-out set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5ba89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are often trying out different model types:\n",
    "- OLS with raw features\n",
    "- OLS with collinear features dropped\n",
    "- OLS with polynomial features\n",
    "- Ridge regressor (will see later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de86f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Idea is that we try out different model types / tune models: \n",
    "- assess variance\n",
    "- assess average performance\n",
    "\n",
    "**Use train/validation for this**: \n",
    "- for each model type: estimate model average performance and variance *across different train/validation realizations*\n",
    "\n",
    "True and final evaluation:\n",
    "- Measure performance on tuned model on the test set that has never been seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0fb52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/cvtuningflow.png\"  width = 800/>\n",
    "<center> Model comparison/selection using cross-validation </center>\n",
    "<center> Best model from cross-validation in test phase</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a88af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Roughly:\n",
    "- Training data is for building the model;\n",
    "- Validation data is for *tweaking* the model;\n",
    "- Testing data is for evaluating the model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f699ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Think of **training** data as what you study for a test\n",
    "- Think of **validation** data is using a practice test (note sometimes called **dev**)\n",
    "- Think of **testing** data as what you use to judge the model\n",
    "    - A **holdout** set is when your test dataset is never used for training (unlike in cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cb3f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Selected best model based on:\n",
    "- what worked best on the given validation folds.\n",
    "\n",
    "**Iterative optimization of models based on the train/validation data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402cc6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ultimately: \n",
    "\n",
    "- want to evaluate our best model class (found by optimizing over the validation sets) \n",
    "- on data that has neither been trained or validated on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fdbf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)\n",
    "> Image from Scikit-Learn https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df73970",
   "metadata": {},
   "source": [
    "\n",
    "<img src = \"Images/test_phase_afterCV.png\"  width = 800/>\n",
    "<center> Best model from cross-validation in test phase</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5ed7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Split data into training data and a holdout test\n",
    "2. Design a model\n",
    "3. Evaluate how well it generalizes with **cross-validation** (only training data)\n",
    "4. Determine if we should adjust model, use cross-validation to evaluate, and repeat\n",
    "5. After iteratively adjusting your model, do a _final_ evaluation with the holdout test set\n",
    "6. DON'T TOUCH THE MODEL!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e957db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross validation gives us a way to test statistical robustness of model performance:\n",
    "- evaluate average performance\n",
    "- evaluate model variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b405d95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But seeing a set of models have high variance:\n",
    "- How to address this problem found in cross-validation trials?\n",
    "- i.e., how do we lower the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6452054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ways to limit/deal with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b9508",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Get more data. With enough training data, even with floppy weights it'll get it right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aae30f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yeah, but often not possible/easy to get enough data for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846a64e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Get rid of columns that exhibit a high degree of collinearity with other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf3b38",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yeah, but did we throw out some useful information for prediction? \n",
    "- ICR and schooling not the same thing.\n",
    "- How many of the collinear columns should we throw away? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea673a95",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Getting rid of columns like this:\n",
    "- Can lower variance\n",
    "- But can also increase bias in an arbitrary, non-optimal way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a679fcd",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Or we could come up with ways to directly limit the variance through the cost function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b47f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hope is that with this method:\n",
    "- decrease variance\n",
    "- without increasing bias too much\n",
    "\n",
    "Doing this in an optimal and principled way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662089c6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58396d23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preventing Overfitting - Regularization\n",
    "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data-generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb4ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When a model has large weights, the model is \"too confident\". This translates to a model with high variance which puts it in danger of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efc955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/punishing_model_metaphor.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75f254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to punish large (confident) weights by contributing them to the error function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8391e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Some Types of Regularization:**\n",
    "\n",
    "1. Reducing the number of features\n",
    "2. Increasing the amount of data\n",
    "3. Popular techniques: Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39232bd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Regularization for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c5d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Modify our squared error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96060411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = |\\textbf{y} - X \\textbf{w}|^2 + \\lambda |\\textbf{w}|^2 $$\n",
    "\n",
    "with $|\\textbf{w}|^2 = w_1^2 + w_2^2 + ... + w_m^2$ as sum of squares of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcd374",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/ridge_regression_geometric.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5cdd1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tug of war between:\n",
    "\n",
    "Ridge cost: $ \\lambda |\\textbf{w}|^2 = \\lambda (w_1^2 + w_2^2)  $\n",
    "- L2 Regularization (Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec07e0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Least squares cost: $ |\\textbf{y} - X\\textbf{w}|^2 $\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fae68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/ridge_regression_geometric.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b2dae",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Penalizes large weights: \n",
    "- **strongly** discourages large fluctuations in $\\textbf{w}$ depending on training set.\n",
    "- i.e. reduces Var[$\\textbf{w}$]\n",
    "- **Can lead to large performance boost on unseen data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89552a9f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the tradeoff: repeating fitting on large number of distinct training sets (samples of population:\n",
    "    - Average of $\\textbf{w}$ is a little off from best fit to population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9c007",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our goal is to construct a model with:\n",
    "- as low a bias as possible (gets close to the true $\\textbf{w}$ if we had/fit on the entire population)\n",
    "- as low a *model* variance as possible (spread in $\\textbf{w}$ is low):\n",
    "    - implies $\\textbf{w}$ is tightly clustered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10570897",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Idea of Ridge: \n",
    "- Tune $\\lambda$ just right. This is something we input as external parameter to model. **Hyperparameter** \n",
    "- Cluster of $\\textbf{w}$ little off the center of the bullseye\n",
    "- But: tightly clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7db96",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With $\\lambda$ tuned well:\n",
    "\n",
    "- not likely to make generalization errors due to large fluctuation in $\\textbf{w}$\n",
    "- But doesnt shift $\\textbf{w}$ too far from least squares estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ee25e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK let's do a Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca8ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# model validation: testing model variance with cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e47efe",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "birds = sns.load_dataset('penguins')\n",
    "birds = birds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85aabe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "birds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a134327",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Going to use the other features to predict the body mass of a penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099fd4d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = birds.drop('body_mass_g', axis=1)\n",
    "y = birds['body_mass_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e781c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83127842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's one-hot encode the nominal categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4e29a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Taking in other features (category)\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "dummies = ohe.fit_transform(X_train[['species', 'island', 'sex']])\n",
    "\n",
    "# Getting a DF\n",
    "X_train_onehot = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "\n",
    "X_train_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072c0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Attach this to dataframe with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8ee37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numeric = X_train[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']]\n",
    "X_train_df = pd.concat([X_train_numeric, X_train_onehot], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd822a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have our training feature matrix:\n",
    "- Apply transformation fit_transformed on train set to test feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79229562",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_dummies = ohe.transform(X_test[['species', 'island', 'sex']])\n",
    "test_df = pd.DataFrame(test_dummies.todense(), columns=ohe.get_feature_names_out(),\n",
    "                       index=X_test.index)\n",
    "X_test_df = pd.concat([X_test[['bill_length_mm', 'bill_depth_mm',\n",
    "                              'flipper_length_mm']], test_df], axis=1)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a171ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "lr1.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b82f9b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lr1.predict(X_test_df)\n",
    "np.sqrt(mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03424149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wow that's a good $R^2$ value!\n",
    "\n",
    "- Estimate how we are doing on unseen data with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967516d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(X=X_train_df, y=y_train, estimator=lr1, cv=10, scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983762c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How well model explains training fold data in each iteration cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12213a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_res = cv_results['train_r2']\n",
    "train_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d2788",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's fitting well each time in the 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613defb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the validation look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29199a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res = cv_results['test_r2']\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17733a51",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612080c5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86b87c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Good average test performance and relatively low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2038f5d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train = np.sqrt(np.abs(cv_results['train_neg_mean_squared_error']))\n",
    "RMSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a855a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcbee8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d9d5c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test = np.sqrt(np.abs(cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695703cf",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly larger average RMSE and variance of RMSE in the test set. But still pretty decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebd585",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac50032",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cfd3e",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But...I'm a greedy man.\n",
    "\n",
    "I want to do better than this. \n",
    "- I'm going to add some polynomials to get a more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb6ed8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "My motivation:\n",
    "    \n",
    "- More complex model = better able to capture more complex relationships between mass and other variables.\n",
    "- Better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31c497",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b555934",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2b759",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=3)\n",
    "X_poly_train = pd.DataFrame(pf.fit_transform(X_train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2492b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly_test = pf.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdce38e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ac70f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56ebaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4423ed",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_lr.score(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f53de1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99352e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "About a 3% improvement: \n",
    "- that could mean money in other contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b2724",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_cv_results = cross_validate(\n",
    "                X=X_poly_train, \n",
    "                y=y_train,\n",
    "                estimator=poly_lr, \n",
    "                cv=10,\n",
    "                scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3720c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res = poly_cv_results['train_r2']\n",
    "poly_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2e53a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346fc76",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res.std(ddof =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99963559",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wow...I'm ready to brag to my boss.\n",
    "\n",
    "- But let's check the performance on the validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a58fa",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res = poly_cv_results['test_r2']\n",
    "poly_valid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64beacb9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bb5c4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c2d92",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest = np.sqrt(np.abs(poly_cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_polytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44768992",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35df06",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e7e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/punch_chuck_norris.gif\" width = 400/></center>\n",
    "<center>You just got punched in the face by the bias-variance problem.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269524",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit an overly complex model:\n",
    "- Doesn't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adb691",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try regularizing polynomial model:\n",
    "$$  L = ||\\textbf{y} - X \\textbf{w}||_2^2 + \\lambda |\\textbf{w}|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c7cf3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- reduce the floppiness/complexity of model\n",
    "- but still keep *some* of the complexity added by these polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec4ee4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reduce Var[$\\textbf{w}$].\n",
    "- Get model predictions more representative of population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186734db",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# You should always be sure to _standardize_ your data before\n",
    "# applying regularization!\n",
    "\n",
    "X_train_processed = pf.fit_transform(ss.fit_transform(X_train_df))\n",
    "X_test_processed = pf.transform(ss.transform(X_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265328c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Absolutely need to standardize/normalize features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6111c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " $$ L = ||\\textbf{y} - X \\textbf{w}||_2^2 + \\lambda |\\textbf{w}|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14509987",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L2 regularization cost function makes no sense otherwise. \n",
    "- Weights will be on different scales if features not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240df57",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 'Lambda' is the standard variable for the strength of the\n",
    "# regularization (as in the above formulas), but since lambda\n",
    "# is a key word in Python, these sklearn regularization tools\n",
    "# use 'alpha' instead.\n",
    "\n",
    "rr = Ridge(alpha=100, random_state=42)\n",
    "\n",
    "rr.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897904a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc1c8a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96142a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let' s cross validate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf4315",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=rr, \n",
    "                cv=10,\n",
    "                scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097e9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Get $R^2$ on train folds of cross validation trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7d719",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['train_r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9891b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Get $R^2$ on validation folds of cross validation trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16b428",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ea9b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0502ae6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2'].std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f668e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest = np.sqrt(np.abs(rr_cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_rrtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6145010",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4796d90",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77155102",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L2 regularized polynomial model:\n",
    "- A little bit worse than my basic linear model.\n",
    "- Much much better than the un-regularized polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059e368",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could I do better? Possibly.\n",
    "\n",
    "- Tune hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fe796",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tuning hyperparameters\n",
    "\n",
    "- Don't know what $\\lambda$ will allow model to perform best on validation sets.\n",
    "- Need to tune this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bb513",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Adjust model stiffness/regularization parameter\n",
    "- Assess model performance in validation testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca4348",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizing the Regularization Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e0dd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The most basic hyperparameter tuning method: Make a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbb9b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The regularization strength could sensibly be any nonnegative number, so there's no way to check \"all possible\" values. It's often useful to try several values that are different orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc79f54",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "cv_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha, random_state=42)\n",
    "    cv_loop_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=rr, \n",
    "                cv=10,\n",
    "                scoring=('neg_mean_squared_error'))\n",
    "    cv_scores.append(np.mean(np.sqrt(np.abs(cv_loop_results['test_score']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ad821",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x = np.log10(alphas), y = cv_scores, marker = 's', ax = ax)\n",
    "ax.set_xlabel('Log(lambda)')\n",
    "ax.set_ylabel('Mean RMSE')\n",
    "ax.set_title('RMSE averaged on validation folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176c935",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could fine tune:\n",
    "- But of hyperparameter values tried $\\lambda = 100$ is best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24158bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we *finally* report results on the true test set:\n",
    "- We have not fit optimized on it.\n",
    "- Have not tuned hyperparameters to see how well it performs on validation folds.\n",
    "\n",
    "**Test/hold-out set is our true final gold standard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9d2b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha = 100, random_state = 42)\n",
    "rr.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea6a91",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = rr.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fac083",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "ridge_RMSE_holdout = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "ridge_RMSE_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7444db",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given the scale of the penguin body mass (g): this is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95174519",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09604ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Takeaways of what we just did"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7030d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Polynomial model: poor prediction performance.\n",
    "- L2 regularized the polynomial regression model (Ridge regression) + tuning\n",
    "- **Much** better test performance than unregularized polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b7f5b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But:\n",
    "    \n",
    "- Our simple linear model with no polynomial worked well.\n",
    "- Almost as well as polynomial features\n",
    "- Think carefully before adding model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb74a08",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A simple model with less number of good quality predictive features may work as well if not better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4e40f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logical extension:\n",
    "- Features that are correlated but don't want to throw them away.\n",
    "- L2 regularized linear model + tuning:\n",
    "    - don't throw out features.\n",
    "    - get better test performance than OLS by reducing weight variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d5e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes though: throwing away features might work better:\n",
    "- Learn good features only with high predictive power\n",
    "- Chuck the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92a67c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### L1 Regularization (LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2b5f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = ||\\textbf{y} - X \\textbf{w}||_2^2 + \\lambda ||\\textbf{w}||_1 $$\n",
    "\n",
    "with $||\\textbf{w}||_1 = |w_1| + |w_2| + ... + |w_m|$ as sum of absolute magnitude of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c66c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Taxi cab vs Euclidean distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d12cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/metrics.png\" width = 450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555609e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Circle in terms of L2 vs L1 distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb0240",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Circle\" in L1:\n",
    "<img src = \"Images/taxcabgeometry.jpg\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb0914",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Circles\" for different metrics\n",
    "\n",
    "<img src = \"Images/circles.png\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c03c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why use the L1 magnitude $||\\textbf{w}||_1$ for regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd08c7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "LASSO encourages model weight sparsity: \n",
    "- prefers to drive weights $w_i$ for features with little predictive power to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c89e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/different_metric_regularization.png\" width = 600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc13e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perform LASSO regression with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05e734",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9719204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Find the best LASSO model: tune regularization hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9aa65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1, 10, 100, 1e3, 1e4]\n",
    "cv_lasso_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter = 10000)\n",
    "    cv_loop_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=lasso, \n",
    "                cv=10,\n",
    "                scoring=('neg_mean_squared_error'))\n",
    "    cv_lasso_scores.append(np.mean(np.sqrt(np.abs(cv_loop_results['test_score']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac7411",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dab90",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_lasso_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4d7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best LASSO model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e20bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_opt = Lasso(alpha=10, random_state=42,  max_iter = 100000)\n",
    "lasso_opt.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = lasso_opt.predict(X_test_processed) # get final test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131c4a9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_RMSE = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "lasso_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832e94a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge_RMSE_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961f1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparable between ridge and LASSO. LASSO tends to have higher weight variance than ridge.\n",
    "\n",
    "But what's the real difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b019d5",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(rr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f96d93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_opt.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286eca2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Strategy Behind Ridge / Lasso / Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbee2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have **overly large coefficients**. \n",
    "\n",
    "Generally, overfitting models come from a result of high model variance. High model variance can be caused by:\n",
    "\n",
    "- having irrelevant or too many predictors\n",
    "- multicollinearity\n",
    "- large coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e39e3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ridge \n",
    "When we introduce many features that:\n",
    "- we believe may all have some predictive power.\n",
    "- want to heavily penalize weight variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc2f51",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b47f3a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have dataset with many highly correlated features:\n",
    "- believe many are not actually adding to predictive power.\n",
    "- willing to cut away marginally unimportant features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab46955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which is better:\n",
    "- depends on dataset\n",
    "- modeling goal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb6962",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEVEL UP - Elastic Net!\n",
    "Naturally, the Elastic Net has the same interface through sklearn as the other regularization tools! The only difference is that we now have to specify how much of each regularization term we want. The name of the parameter for this (represented by $\\rho$ above) in sklearn is `l1_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddf857",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=10, l1_ratio=0.1, random_state=42)\n",
    "\n",
    "enet.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee0ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enet.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b4e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enet.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ce0cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Setting the `l1_ratio` to 1 is equivalent to the lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80c713",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ratios = np.linspace(0.01, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a65c50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for ratio in ratios:\n",
    "    enet = ElasticNet(alpha=100, l1_ratio=ratio, random_state=42)\n",
    "    enet.fit(X_train_processed, y_train)\n",
    "    preds.append(enet.predict(X_test_processed[0].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9335e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "lasso = Lasso(alpha=100, random_state=42)\n",
    "lasso.fit(X_train_processed, y_train)\n",
    "lasso_pred = lasso.predict(X_test_processed[0].reshape(1, -1))\n",
    "\n",
    "ax.plot(ratios, preds, label='elastic net')\n",
    "ax.scatter(1, lasso_pred, c='k', s=70, label='lasso')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab58565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Note on `ElasticNet()`\n",
    "Is an Elastic Net with `l1_ratio` set to 0 equivalent to the ridge? In theory yes. But in practice no. It looks like the `ElasticNet()` predictions on the first test data point as `l1_ratio` shrinks are tending toward some value around 3400. Let's check to see what prediction `Ridge()` gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d527066",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10, random_state=42)\n",
    "ridge.fit(X_train_processed, y_train)\n",
    "ridge.predict(X_test_processed[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c1571",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you check the docstring for the `ElasticNet()` class you will see:\n",
    "- that the function being minimized is slightly different from what we saw above; and\n",
    "- that the results are unreliable when `l1_ratio` $\\leq 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**: Visualize the difference in this case between `ElasticNet(l1_ratio=0.01)` and `Ridge()` by making a scatterplot of each model's predicted values for the first ten points in `X_test_processed`. Use `alpha=10` for each model.\n",
    "\n",
    "        Level Up: Make a second scatterplot that compares the predictions on the same data\n",
    "        points between ElasticNet(l1_ratio=1) and Lasso()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7e2bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    <code>fig, ax = plt.subplots()\n",
    "enet_r = ElasticNet(alpha=10, l1_ratio=0.01, random_state=42)\n",
    "enet_r.fit(X_train_processed, y_train)\n",
    "preds_enr = enet_r.predict(X_test_processed[:10])\n",
    "preds_ridge = ridge.predict(X_test_processed[:10])\n",
    "ax.scatter(np.arange(10), preds_enr)\n",
    "ax.scatter(np.arange(10), preds_ridge);</code>  \n",
    "        </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2484d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        Level Up Answer\n",
    "    </summary>\n",
    "<code>fig, ax = plt.subplots()\n",
    "enet_l = ElasticNet(alpha=10, l1_ratio=1, random_state=42)\n",
    "enet_l.fit(X_train_processed, y_train)\n",
    "preds_enl = enet_l.predict(X_test_processed[:10])\n",
    "preds_lasso = lasso.predict(X_test_processed[:10])\n",
    "ax.scatter(np.arange(10), preds_enl)\n",
    "ax.scatter(np.arange(10), preds_lasso);</code>\n",
    "    </details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54323b69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fitting Regularized Models with Cross-Validation\n",
    "Our friend `sklearn` also includes tools that fit regularized regressions *with cross-validation*: `LassoCV`, `RidgeCV`, and `ElasticNetCV`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc49486",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Use `RidgeCV` to fit a seven-fold cross-validated ridge regression model to our `X_train_processed` data and then calculate $R^2$ and the RMSE (root-mean-squared error) on our test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245e82e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        Answer\n",
    "    </summary>\n",
    "    <code>rcv = RidgeCV(cv=7)\n",
    "rcv.fit(X_train_processed, y_train)\n",
    "rcv.score(X_test_processed, y_test)\n",
    "np.sqrt(mean_squared_error(y_test, rcv.predict(X_test_processed)))</code>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc07f8",
   "metadata": {},
   "source": [
    "## Level Up Exercise: Name that Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e42230",
   "metadata": {},
   "source": [
    "Consider the following scenarios and describe them according to bias and variance. There are four possibilities:\n",
    "\n",
    "- a. The model has low bias and high variance.\n",
    "- b. The model has high bias and low variance.\n",
    "- c. The model has both low bias and low variance.\n",
    "- d. The model has both high bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e277ce2",
   "metadata": {},
   "source": [
    "**Scenario 1**: The model has a low RMSE on training and a low RMSE on test.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef5606",
   "metadata": {},
   "source": [
    "**Scenario 2**: The model has a high $R^2$ on the training set, but a low $R^2$ on the test.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559532e",
   "metadata": {},
   "source": [
    "**Scenario 3**: The model performs well on data it is fit on and well on data it has not seen.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b5190",
   "metadata": {},
   "source": [
    "**Scenario 4**: The model has a low $R^2$ on training but high on the test set.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    d. The model has both high bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8880d",
   "metadata": {},
   "source": [
    "**Scenario 5**: The model leaves out many of the meaningful predictors, but is consistent across samples.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    b. The model has high bias and low variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7237ee2",
   "metadata": {},
   "source": [
    "**Scenario 6**: The model is highly sensitive to random noise in the training set.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1968923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
